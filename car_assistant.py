#!/usr/bin/env python3
"""
è»Šè¼‰èªéŸ³åŠ©ç† (Voice Car Assistant) - åŸºæ–¼ local-talking-llm (ç°¡åŒ–ç‰ˆ)
æ”¯æ´ä¸­è‹±æ–‡èªéŸ³è­˜åˆ¥ï¼Œä½¿ç”¨ faster-whisperï¼Œç´”æ–‡å­—å›è¦†ï¼ˆç„¡ TTSï¼‰
"""

import time
import threading
import numpy as np
from faster_whisper import WhisperModel
import sounddevice as sd
import argparse
from rich.console import Console
import asyncio
import httpx
from silero_vad import load_silero_vad, get_speech_timestamps
import openai
import os

# ç¾ä»£åŒ–çš„LangChainå°å…¥
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.chat_history import InMemoryChatMessageHistory
from langchain_ollama import OllamaLLM

system_prompt = """ä½ æ˜¯è»Šè¼‰æ™ºæ…§åŠ©ç†ã€‚å°ˆç‚ºå°ç£é§•é§›è€…è¨­è¨ˆï¼Œå¯è™•ç†ä¸­æ–‡è»Šè¼‰æŒ‡ä»¤ã€‚

å®‰å…¨åŸå‰‡ï¼š
- çµ•å°ç¦æ­¢è‡ªå‹•é§•é§›æŒ‡ä»¤
- ä¸åŸ·è¡Œå±éšªæˆ–éæ³•æ“ä½œ
- å°ç£åœ°å€ä»¥å¤–çš„å°èˆªè«‹æ±‚å°‡è¢«æ‹’çµ•

å›å¾©æ ¼å¼ï¼šç°¡æ½”ã€ç›´æ¥ï¼Œç¬¦åˆé§•é§›æƒ…å¢ƒ

è¼¸å‡ºè¦å‰‡ï¼š
1. ä¸»è¦å›æ‡‰ï¼š<message>ç°¡çŸ­å›è¦†ç”¨æˆ¶(15å­—å…§)</message>
2. æŒ‡ä»¤åŸ·è¡Œï¼š<command>FUNCTION_NAME(åƒæ•¸)</command>
3. éŒ¯èª¤è™•ç†ï¼š<error>éŒ¯èª¤æè¿°</error>

æ”¯æ´æŒ‡ä»¤ï¼š
- OpenWindow(zone="ä½ç½®", value="ç‹€æ…‹") # ä½ç½®: FRONT_LEFT, FRONT_RIGHT, REAR_LEFT, REAR_RIGHT; ç‹€æ…‹: open, close
- SetFanSpeed(zone="å€åŸŸ", value=æ•¸å€¼) # å€åŸŸ: DRIVER, PASSENGER, REAR, ALL; æ•¸å€¼: 1-5
- SetNavigation(destination="ç›®çš„åœ°", type="é¡å‹") # é¡å‹: address, poi, home, work
- SetTemperature(zone="å€åŸŸ", value=æº«åº¦) # å€åŸŸ: DRIVER, PASSENGER, REAR, ALL; æº«åº¦: 16-32
- PlayMusic(action="å‹•ä½œ", target="ç›®æ¨™") # å‹•ä½œ: play, pause, next, previous; ç›®æ¨™: æ­Œå/è—äºº
- MakeCall(contact="è¯çµ¡äºº") # æ’¥æ‰“é›»è©±
- SendMessage(contact="è¯çµ¡äºº", message="å…§å®¹") # ç™¼é€è¨Šæ¯

ç¯„ä¾‹æ ¼å¼ï¼š
ç”¨æˆ¶ï¼š"é–‹å‰å·¦è»Šçª—"
<message>æ­£åœ¨é–‹å•Ÿè»Šçª—</message>
<command>OpenWindow(zone="FRONT_LEFT", value="open")</command>

ç”¨æˆ¶ï¼š"è¨­å®šæº«åº¦22åº¦"
<message>èª¿æ•´æº«åº¦ä¸­</message>
<command>SetTemperature(zone="ALL", value=22.0)</command>

è«‹åš´æ ¼æŒ‰ç…§æ­¤æ ¼å¼å›æ‡‰ã€‚"""

console = Console()

class CarVoiceAssistant:
    def __init__(self, whisper_model="medium", ollama_model="qwen2.5:3b", use_openai=False):
        self.whisper_model = whisper_model
        self.ollama_model = ollama_model
        self.use_openai = use_openai
        self.stt = None  # faster-whisper æ¨¡å‹
        self.llm = None
        self.chain_with_history = None
        self.chat_sessions = {}
        
        # OpenAIå®¢æˆ¶ç«¯è¨­ç½® - ä¸€å¾‹å¾ç’°å¢ƒè®Šæ•¸ç²å–APIå¯†é‘°
        if self.use_openai:
            self.openai_api_key = os.getenv('OPENAI_API_KEY')
            if self.openai_api_key:
                openai.api_key = self.openai_api_key
                self.openai_client = openai.OpenAI(api_key=self.openai_api_key)
                console.print("[green]âœ… ç’°å¢ƒè®Šæ•¸è¼‰å…¥")
            else:
                console.print("[red]âŒ ç’°å¢ƒè®Šæ•¸ OPENAI_API_KEY æœªè¨­ç½®ï¼Œå°‡åƒ…ä½¿ç”¨æœ¬åœ°æ¨¡å‹")
                self.use_openai = False
        
        # VADç›¸é—œ
        self.vad_model = None
        self.is_listening = False
        self.audio_buffer = []
        self.sample_rate = 16000
        self.silence_duration = 1.0
        
        # è»Šè¼‰ç³»çµ±ç‹€æ…‹
        self.vehicle_state = {
            "speed": 0,
            "fuel_level": 75,
            "engine_temp": 90,
            "current_location": "å°åŒ—å¸‚ä¿¡ç¾©å€",
            "destination": None,
            "traffic_condition": "æ­£å¸¸",
            "weather": "æ™´æœ—"
        }
        
    async def initialize(self):
        """åˆå§‹åŒ–æ‰€æœ‰çµ„ä»¶ - ä½¿ç”¨ faster-whisper"""
        console.print("[cyan]ğŸš— è»Šè¼‰æ™ºæ…§åŠ©ç†åˆå§‹åŒ–ä¸­...")
        
        # è¼‰å…¥VADæ¨¡å‹
        console.print("[yellow]è¼‰å…¥VADèªéŸ³æ´»å‹•æª¢æ¸¬æ¨¡å‹...")
        try:
            vad_result = load_silero_vad(onnx=True)
            if isinstance(vad_result, tuple):
                self.vad_model, _ = vad_result
            else:
                self.vad_model = vad_result
            console.print("[green]âœ… VADæ¨¡å‹è¼‰å…¥æˆåŠŸ")
        except Exception as e:
            console.print(f"[red]âŒ VADæ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
            console.print("[yellow]å°‡ä½¿ç”¨fallbackæ¨¡å¼ï¼ˆç„¡VADï¼‰")
        
        # è¼‰å…¥ Faster-Whisper æ¨¡å‹
        console.print(f"[yellow]è¼‰å…¥ Faster-Whisper æ¨¡å‹: {self.whisper_model}")
        try:
            self.stt = WhisperModel(
                self.whisper_model,
                device="cpu",
                compute_type="int8",
                cpu_threads=0,
                num_workers=1
            )
            console.print("[green]âœ… Faster-Whisper è¼‰å…¥æˆåŠŸ")
        except Exception as e:
            console.print(f"[red]âŒ Faster-Whisper è¼‰å…¥å¤±æ•—: {e}")
            raise
        
        # æ¸¬è©¦Ollamaé€£æ¥
        await self.test_ollama_connection()
        
        # åˆå§‹åŒ–LLM
        self.setup_llm()
        
        console.print("[green]âœ… æ™ºæ…§åŠ©ç†åˆå§‹åŒ–å®Œæˆ")
        console.print("[cyan]ğŸ¤ ç³»çµ±èƒŒæ™¯ç›£è½ä¸­...")
        
    async def test_ollama_connection(self):
        """æ¸¬è©¦Ollamaæœå‹™é€£æ¥"""
        try:
            async with httpx.AsyncClient() as client:
                response = await client.get("http://localhost:11434/api/tags")
                if response.status_code == 200:
                    models = response.json()
                    console.print("[green]âœ… Ollamaæœå‹™é€£æ¥æˆåŠŸ")
                    available_models = [m['name'] for m in models.get('models', [])]
                    if self.ollama_model not in available_models:
                        console.print(f"[yellow]âš ï¸ æ¨¡å‹ {self.ollama_model} æœªæ‰¾åˆ°")
                        if available_models:
                            console.print(f"[blue]å¯ç”¨æ¨¡å‹: {', '.join(available_models[:3])}")
                            # ä½¿ç”¨ç¬¬ä¸€å€‹å¯ç”¨æ¨¡å‹
                            self.ollama_model = available_models[0]
                            console.print(f"[blue]å°‡ä½¿ç”¨: {self.ollama_model}")
                else:
                    raise Exception(f"HTTP {response.status_code}")
        except Exception as e:
            console.print(f"[red]âŒ Ollamaé€£æ¥å¤±æ•—: {e}")
            console.print("[yellow]è«‹ç¢ºä¿Ollamaæœå‹™å·²å•Ÿå‹•: ollama serve")
            raise
    
    def setup_llm(self):
        """è¨­ç½®LLMå°è©±éˆ"""
        prompt_template = ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            MessagesPlaceholder(variable_name="history"),
            ("human", "{input}")
        ])
        
        # åˆå§‹åŒ–LLM
        self.llm = OllamaLLM(model=self.ollama_model, base_url="http://localhost:11434")
        
        # å‰µå»ºå°è©±éˆ
        chain = prompt_template | self.llm
        
        # æ·»åŠ æ­·å²è¨˜éŒ„åŠŸèƒ½
        self.chain_with_history = RunnableWithMessageHistory(
            chain,
            self.get_session_history,
            input_messages_key="input",
            history_messages_key="history",
        )
    
    def get_session_history(self, session_id: str) -> InMemoryChatMessageHistory:
        """ç²å–æˆ–å‰µå»ºå°è©±æ­·å²"""
        if session_id not in self.chat_sessions:
            self.chat_sessions[session_id] = InMemoryChatMessageHistory()
        return self.chat_sessions[session_id]
    
    def transcribe_chinese(self, audio_np: np.ndarray) -> str:
        """ä½¿ç”¨ faster-whisper è½‰éŒ„éŸ³è¨Šç‚ºä¸­è‹±æ–‡æ–‡å­—"""
        try:
            # ä½¿ç”¨ faster-whisper é€²è¡Œè½‰éŒ„ï¼Œæ”¯æ´ä¸­è‹±æ–‡
            segments, info = self.stt.transcribe(
                audio_np,
                language=None,  # è‡ªå‹•æª¢æ¸¬èªè¨€ï¼ˆä¸­è‹±æ–‡ï¼‰
                task="transcribe",
                temperature=0.0,
                vad_filter=True,
                vad_parameters=dict(
                    min_silence_duration_ms=500,
                    threshold=0.5,
                    max_speech_duration_s=30,
                    min_speech_duration_ms=250
                ),
                condition_on_previous_text=False,
                compression_ratio_threshold=2.4,
                log_prob_threshold=-1.0,
                no_speech_threshold=0.6
            )
            
            # çµ„åˆæ‰€æœ‰ç‰‡æ®µ
            text_parts = []
            for segment in segments:
                text_parts.append(segment.text.strip())
            text = " ".join(text_parts).strip()
            
            # éæ¿¾æ˜é¡¯éŒ¯èª¤çš„è­˜åˆ¥çµæœ
            if len(text) < 2:
                return ""
                
            # éæ¿¾å¸¸è¦‹èª¤è­˜åˆ¥å…§å®¹
            ignore_phrases = ["è¬è¬è§€çœ‹", "è«‹è¨‚é–±", "æ„Ÿè¬æ”¶çœ‹", "å­—å¹•", "è«‹é—œæ³¨", "é»è®š"]
            if any(phrase in text for phrase in ignore_phrases):
                return ""
                
            return text
        except Exception as e:
            console.print(f"[red]èªéŸ³è½‰éŒ„éŒ¯èª¤: {e}")
            return ""

    async def get_openai_response(self, text: str) -> str:
        """ç²å–OpenAI GPT-4o-miniå›æ‡‰"""
        try:
            response = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: self.openai_client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": text}
                    ],
                    temperature=0.7,
                    max_tokens=200
                )
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            console.print(f"[red]OpenAI APIéŒ¯èª¤: {e}")
            return "æŠ±æ­‰ï¼ŒOpenAIæœå‹™æš«æ™‚ç„¡æ³•å›æ‡‰æ‚¨çš„è«‹æ±‚ã€‚"

    async def get_llm_response(self, text: str) -> str:
        """ç²å–LLMå›æ‡‰"""
        try:
            session_id = "car_assistant_session"
            
            # èª¿ç”¨å°è©±éˆ
            response = await asyncio.get_event_loop().run_in_executor(
                None, 
                lambda: self.chain_with_history.invoke(
                    {"input": text},
                    config={"session_id": session_id}
                )
            )
            
            return response.strip()
            
        except Exception as e:
            console.print(f"[red]LLMå›æ‡‰éŒ¯èª¤: {e}")
            return "æŠ±æ­‰ï¼Œç³»çµ±æš«æ™‚ç„¡æ³•å›æ‡‰æ‚¨çš„è«‹æ±‚ã€‚"
    
    async def get_dual_response(self, text: str) -> tuple:
        """åŒæ™‚ç²å–æœ¬åœ°ollamaå’ŒOpenAIçš„å›æ‡‰"""
        tasks = []
        
        # æ·»åŠ æœ¬åœ°ollamaä»»å‹™
        tasks.append(("Ollama", self.get_llm_response(text)))
        
        # å¦‚æœå•Ÿç”¨OpenAIï¼Œæ·»åŠ OpenAIä»»å‹™
        if self.use_openai:
            tasks.append(("OpenAI", self.get_openai_response(text)))
        
        # ä¸¦è¡ŒåŸ·è¡Œæ‰€æœ‰ä»»å‹™
        results = []
        for name, task in tasks:
            try:
                result = await task
                results.append((name, result))
            except Exception as e:
                results.append((name, f"éŒ¯èª¤: {e}"))
        
        return results

    def continuous_audio_monitoring(self, stop_event):
        """é€£çºŒéŸ³è¨Šç›£æ§å’ŒVADæª¢æ¸¬ - Windowså„ªåŒ–ç‰ˆ"""
        self.audio_buffer = []
        silence_start = None
        recording = False
        noise_samples = []
        volume_history = []
        
        def audio_callback(indata, frames, time, status):
            nonlocal recording, silence_start, noise_samples, volume_history
            if status:
                console.print(f"[yellow]éŸ³è¨Šç‹€æ…‹: {status}")
            
            # å°‡éŸ³è¨Šæ•¸æ“šè½‰æ›ç‚ºnumpyæ•¸çµ„
            audio_data = np.frombuffer(indata, dtype=np.int16).astype(np.float32) / 32768.0
            
            # ä½¿ç”¨éŸ³é‡æª¢æ¸¬
            volume = np.sqrt(np.mean(audio_data ** 2))  # RMSéŸ³é‡
            volume_threshold = 0.01  # å›å¾©åŸå§‹ä½é–¾å€¼
            
            if self.vad_model is not None:
                # ä½¿ç”¨VADæª¢æ¸¬èªéŸ³
                try:
                    if volume > volume_threshold and len(audio_data) >= 512:
                        speech_timestamps = get_speech_timestamps(
                            audio_data, 
                            self.vad_model, 
                            sampling_rate=self.sample_rate,
                            threshold=0.3,
                            min_speech_duration_ms=100,
                            min_silence_duration_ms=100
                        )
                        has_speech = len(speech_timestamps) > 0
                    else:
                        has_speech = volume > volume_threshold
                        
                    if has_speech:
                        if not recording:
                            console.print(f"[green]ğŸ¤ æª¢æ¸¬åˆ°èªéŸ³(éŸ³é‡:{volume:.4f})ï¼Œé–‹å§‹éŒ„éŸ³...")
                            recording = True
                            self.audio_buffer = []
                        self.audio_buffer.extend(audio_data)
                        silence_start = None
                    else:
                        if recording:
                            if silence_start is None:
                                silence_start = time.inputBufferAdcTime
                            elif time.inputBufferAdcTime - silence_start > self.silence_duration:
                                console.print("[blue]ğŸ”‡ æª¢æ¸¬åˆ°éœéŸ³ï¼Œåœæ­¢éŒ„éŸ³")
                                recording = False
                                stop_event.set()
                except Exception as e:
                    console.print(f"[yellow]VADéŒ¯èª¤ï¼Œä½¿ç”¨éŸ³é‡æª¢æ¸¬: {e}")
                    # å›é€€åˆ°éŸ³é‡æª¢æ¸¬
                    has_speech = volume > volume_threshold
                    if has_speech and not recording:
                        recording = True
                        console.print(f"[yellow]ğŸ“± éŸ³é‡æª¢æ¸¬æ¨¡å¼(éŸ³é‡:{volume:.4f})")
                        self.audio_buffer = []
                    if recording:
                        self.audio_buffer.extend(audio_data)
                        if len(self.audio_buffer) > self.sample_rate * 3:
                            stop_event.set()
            else:
                # ç„¡VADæ™‚ä½¿ç”¨ç´”éŸ³é‡æª¢æ¸¬
                has_speech = volume > volume_threshold
                if has_speech and not recording:
                    recording = True
                    console.print(f"[yellow]ğŸ“± éŸ³é‡æª¢æ¸¬é–‹å§‹éŒ„éŸ³(éŸ³é‡:{volume:.4f})...")
                    self.audio_buffer = []
                if recording:
                    self.audio_buffer.extend(audio_data)
                    if len(self.audio_buffer) > self.sample_rate * 3:
                        stop_event.set()

        # é–‹å§‹éŸ³è¨ŠéŒ„è£½
        with sd.RawInputStream(
            samplerate=self.sample_rate, 
            dtype="int16", 
            channels=1, 
            callback=audio_callback
        ):
            while not stop_event.is_set():
                time.sleep(0.1)

    async def run_assistant(self):
        """é‹è¡Œè»Šè¼‰èªéŸ³åŠ©ç†ä¸»å¾ªç’°"""
        console.print("[cyan]â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        console.print("[cyan]ğŸš— è»Šè¼‰èªéŸ³åŠ©ç†å·²å•Ÿå‹•")
        console.print("[cyan]ğŸ¯ Faster-Whisper ä¸­è‹±æ–‡èªéŸ³è­˜åˆ¥")
        console.print("[cyan]ğŸ“ ç´”æ–‡å­—å›è¦†æ¨¡å¼ (ç„¡TTS)")
        console.print("[cyan]ğŸ¤ ç³»çµ±æ­£åœ¨ç›£è½æ‚¨çš„èªéŸ³")
        console.print("[cyan]ğŸ“± æŒ‰Ctrl+Cé€€å‡ºç³»çµ±")
        console.print("[cyan]â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")

        try:
            while True:
                console.print("[blue]ğŸ” VAD èªéŸ³åµæ¸¬ä¸­...")
                
                # é–‹å§‹é€£çºŒèªéŸ³ç›£æ§
                stop_event = threading.Event()
                monitoring_thread = threading.Thread(
                    target=self.continuous_audio_monitoring,
                    args=(stop_event,),
                )
                monitoring_thread.start()
                monitoring_thread.join()
                
                # ç•¶VADæª¢æ¸¬åˆ°èªéŸ³çµæŸæ™‚ï¼Œè™•ç†éŸ³è¨Š
                audio_np = np.array(self.audio_buffer)
                
                if len(audio_np) > 8000:  # ç¢ºä¿æœ‰è¶³å¤ çš„éŸ³è¨Šæ•¸æ“š
                    # èªéŸ³è½‰æ–‡å­—
                    with console.status("ğŸ¯ èªéŸ³è­˜åˆ¥è™•ç†ä¸­...", spinner="dots"):
                        text = self.transcribe_chinese(audio_np)
                    
                    if text:
                        console.print(f"[yellow]ğŸ‘¤ æ‚¨èªª: {text}")
                        
                        # ç”Ÿæˆå›æ‡‰
                        if self.use_openai:
                            with console.status("ğŸ¤– é›™æ¨¡å‹åŠ©ç†æ€è€ƒä¸­...", spinner="dots"):
                                responses = await self.get_dual_response(text)
                            
                            # é¡¯ç¤ºé›™æ¨¡å‹å›æ‡‰
                            console.print("[cyan]ğŸ¤– åŠ©ç†å›æ‡‰:")
                            for model_name, response in responses:
                                console.print(f"[bold green]{model_name}:[/bold green] {response}")
                                console.print("[dim]â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€[/dim]")
                        else:
                            with console.status("ğŸ¤– åŠ©ç†æ€è€ƒä¸­...", spinner="dots"):
                                response = await self.get_llm_response(text)
                            
                            # é¡¯ç¤ºå–®ä¸€æ¨¡å‹å›æ‡‰
                            console.print(f"[cyan]ğŸ¤– åŠ©ç†: {response}")
                            console.print("[dim]â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€[/dim]")
                    else:
                        console.print("[red]âŒ æœªèƒ½è­˜åˆ¥èªéŸ³ï¼Œè«‹é‡è©¦")
                else:
                    console.print("[yellow]âš ï¸ éŸ³è¨Šå¤ªçŸ­ï¼Œè«‹å†è©¦ä¸€æ¬¡")
                    
        except KeyboardInterrupt:
            console.print("\n[yellow]ğŸ‘‹ æ­£åœ¨é—œé–‰è»Šè¼‰èªéŸ³åŠ©ç†...")
        except Exception as e:
            console.print(f"[red]âŒ ç³»çµ±éŒ¯èª¤: {e}")

def main():
    """ä¸»ç¨‹å¼å…¥å£"""
    parser = argparse.ArgumentParser(description="è»Šè¼‰æ™ºæ…§åŠ©ç† - ä¸­è‹±æ–‡èªéŸ³è­˜åˆ¥ + æ–‡å­—å›è¦†")
    parser.add_argument("--whisper-model", default="medium", 
                       choices=["tiny", "base", "small", "medium", "large"],
                       help="Whisperæ¨¡å‹å¤§å° (é è¨­: mediumï¼Œæ”¯æ´ä¸­è‹±æ–‡)")
    parser.add_argument("--ollama-model", default="qwen2.5:3b",
                       help="Ollamaæ¨¡å‹åç¨± (é è¨­: qwen2.5:3b)")
    parser.add_argument("--use-openai", action="store_true",
                       help="å•Ÿç”¨OpenAI GPT-4o-miniä½œç‚ºç¬¬äºŒå€‹å›æ‡‰ä¾†æº (éœ€è¨­å®š OPENAI_API_KEY ç’°å¢ƒè®Šæ•¸)")
    
    args = parser.parse_args()
    
    # å‰µå»ºè»Šè¼‰åŠ©ç†å¯¦ä¾‹
    assistant = CarVoiceAssistant(
        whisper_model=args.whisper_model,
        ollama_model=args.ollama_model,
        use_openai=args.use_openai
    )
    
    async def run():
        await assistant.initialize()
        await assistant.run_assistant()
    
    # é‹è¡ŒåŠ©ç†
    try:
        asyncio.run(run())
    except KeyboardInterrupt:
        console.print("\n[blue]ğŸš— è»Šè¼‰èªéŸ³åŠ©ç†å·²é—œé–‰ï¼Œç¥æ‚¨è¡Œè»Šå®‰å…¨ï¼")

if __name__ == "__main__":
    main()